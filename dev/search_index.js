var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Still-empty...-TODO.","page":"API Reference","title":"Still empty... TODO.","text":"","category":"section"},{"location":"#StreamCallbacksExt.jl","page":"Home","title":"StreamCallbacksExt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for StreamCallbacksExt.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Extension package for StreamCallbacks.jl with 3 things in mind:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Instant feedback as soon as usage package comes in.\nCached token informations\nCached token costs","category":"page"},{"location":"","page":"Home","title":"Home","text":"also adding timing for prompt processing time and inference time. Primarily designed to extend streaming capabilities in PromptingTools.jl.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Token counting for input, output, and cache operations\nCost calculation for different LLM providers (OpenAI, Anthropic)\nTiming information for inference and message processing\nCustomizable token and content formatters\nSupport for different stream flavors (OpenAI, Anthropic)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg Pkg.add(\"git@github.com:Sixzero/StreamCallbacksExt.jl.git\")","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using PromptingTools, StreamCallbacks, StreamCallbacksExt const PT = PromptingTools","category":"page"},{"location":"#Basic-streaming-with-token-counting","page":"Home","title":"Basic streaming with token counting","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     out = stdout,  # or any IO     flavor = StreamCallbacks.OpenAIStream(), ) msg = aigenerate(\"Write a story about a space cat\";     streamcallback=cb,     model=\"gpt4om\",     api_kwargs=(stream=true,) )","category":"page"},{"location":"#Token-Formatters","page":"Home","title":"Token Formatters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Two built-in formatters are provided:","category":"page"},{"location":"#Default-Text-Format","page":"Home","title":"Default Text Format","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream(),     tokenformatter = defaulttokenformatter ) [in=10, out=20, cachein=2, cache_read=5, $0.0015, 1.234s]","category":"page"},{"location":"#Compact-Emoji-Format","page":"Home","title":"Compact Emoji Format","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream(),     tokenformatter = compacttoken_formatter ) [üî§ in:10 out:20 cache:(w:2,r:5) üí∞$0.0015 ‚ö°Ô∏è1.234s]","category":"page"},{"location":"#Custom-Formatter","page":"Home","title":"Custom Formatter","text":"","category":"section"},{"location":"#Create-your-own-formatter","page":"Home","title":"Create your own formatter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"myformatter(tokens, cost, elapsed) = \"Words: $(tokens.output/1.3) Cost: \\$$(round(cost; digits=4))\" cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream(),     tokenformatter = my_formatter )","category":"page"},{"location":"#Types","page":"Home","title":"Types","text":"","category":"section"},{"location":"#TokenCounts","page":"Home","title":"TokenCounts","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tracks token usage across different categories:","category":"page"},{"location":"","page":"Home","title":"Home","text":"input: Number of new tokens in the prompt (excluding cached tokens)\noutput: Number of generated tokens in the response\ncache_write: Number of tokens written to cache\ncache_read: Number of tokens read from cache","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: Total input tokens = input + cachewrite + cacheread","category":"page"},{"location":"#TimingInfo","page":"Home","title":"TimingInfo","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tracks timing-related information during the streaming process:","category":"page"},{"location":"","page":"Home","title":"Home","text":"creation_time: When the callback was created\ninference_start: When the model started processing\nlast_message_time: Timestamp of the last received message","category":"page"},{"location":"#StreamCallbackWithTokencounts","page":"Home","title":"StreamCallbackWithTokencounts","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Main callback type that implements token counting, cost tracking, and timing: StreamCallbackWithTokencounts(;     out = stdout,              # Output IO stream     flavor = nothing,          # Stream format handler (OpenAI/Anthropic)     totaltokens = TokenCounts(), # Accumulated token counts     model = nothing,           # Model identifier     tokenformatter = defaulttokenformatter,     contentformatter = defaultcontent_formatter,     timing = TimingInfo() )","category":"page"},{"location":"#Cost-Calculation","page":"Home","title":"Cost Calculation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package automatically calculates costs based on the model and provider:","category":"page"},{"location":"#OpenAI-models","page":"Home","title":"OpenAI models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream() )","category":"page"},{"location":"#Anthropic-models","page":"Home","title":"Anthropic models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.AnthropicStream() )","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each provider has specific cache multipliers for cost calculation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"OpenAI: cachewrite: 1.0x, cacheread: 0.5x\nAnthropic: cachewrite: 1.25x, cacheread: 0.1x","category":"page"},{"location":"#Dependencies","page":"Home","title":"Dependencies","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StreamCallbacks.jl\nPromptingTools.jl","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TokenCounts\nStreamCallbacksExt.extract_model(::StreamCallbacks.AnthropicStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_model(::StreamCallbacks.OpenAIStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_model(::StreamCallbacks.AbstractStreamFlavor, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.get_cost(::StreamCallbacks.OpenAIStream, ::String, ::Any)\nStreamCallbacksExt.get_cost(::StreamCallbacks.AnthropicStream, ::String, ::Any)\nStreamCallbacksExt.TimingInfo\nStreamCallbacksExt.default_content_formatter\nStreamCallbacksExt.extract_tokens(::StreamCallbacks.AnthropicStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_tokens(::StreamCallbacks.OpenAIStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_tokens(::StreamCallbacks.AbstractStreamFlavor, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.compact_token_formatter\nStreamCallbacksExt.calculate_cost\nStreamCallbacks.callback(::StreamCallbackWithTokencounts, ::StreamCallbacks.StreamChunk)\nStreamCallbacksExt.default_token_formatter\nStreamCallbacksExt.StreamCallbackWithTokencounts","category":"page"},{"location":"#StreamCallbacksExt.TokenCounts","page":"Home","title":"StreamCallbacksExt.TokenCounts","text":"TokenCounts(; input=0, output=0, cache_write=0, cache_read=0)\n\nTracks token usage across different categories:\n\ninput: Number of new tokens in prompt (excluding cached tokens)\noutput: Number of generated tokens in response\ncache_write: Number of tokens written to cache\ncache_read: Number of tokens read from cache\n\nNote: Total input tokens = input + cachewrite + cacheread\n\n\n\n\n\n","category":"type"},{"location":"#StreamCallbacksExt.extract_model-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_model","text":"extract_model(::StreamCallbacks.AnthropicStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract model identifier from Anthropic stream chunks, specifically from message_start events.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_model-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_model","text":"extract_model(::StreamCallbacks.OpenAIStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract model identifier from OpenAI stream chunks.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_model-Tuple{StreamCallbacks.AbstractStreamFlavor, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_model","text":"extract_model(::StreamCallbacks.AbstractStreamFlavor, chunk::StreamCallbacks.AbstractStreamChunk)\n\nDefault model extractor that warns about unimplemented flavors.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.get_cost-Tuple{OpenAIStream, String, Any}","page":"Home","title":"StreamCallbacksExt.get_cost","text":"get_cost(::StreamCallbacks.OpenAIStream, model::String, tokens)\n\nCalculate costs for OpenAI models with their specific cache multipliers.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.get_cost-Tuple{AnthropicStream, String, Any}","page":"Home","title":"StreamCallbacksExt.get_cost","text":"get_cost(::StreamCallbacks.AnthropicStream, model::String, tokens)\n\nCalculate costs for Anthropic models with their specific cache multipliers.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.TimingInfo","page":"Home","title":"StreamCallbacksExt.TimingInfo","text":"TimingInfo(; creation_time=time(), inference_start=nothing, last_message_time=nothing)\n\nTracks timing information during the streaming process:\n\ncreation_time: When the callback was created\ninference_start: When the model started processing\nlast_message_time: Timestamp of the last received message\n\n\n\n\n\n","category":"type"},{"location":"#StreamCallbacksExt.default_content_formatter","page":"Home","title":"StreamCallbacksExt.default_content_formatter","text":"default_content_formatter(text::AbstractString)\n\nDefault content formatter that returns the text unchanged.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.extract_tokens-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_tokens","text":"extract_tokens(::StreamCallbacks.AnthropicStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract token counts from Anthropic stream chunks. Handles both message_start events with usage information and completion events with output tokens.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_tokens-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_tokens","text":"extract_tokens(::StreamCallbacks.OpenAIStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract token counts from OpenAI stream chunks. Handles both legacy and new token counting formats, including cache hit/miss statistics.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_tokens-Tuple{StreamCallbacks.AbstractStreamFlavor, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_tokens","text":"extract_tokens(::StreamCallbacks.AbstractStreamFlavor, chunk::StreamCallbacks.AbstractStreamChunk)\n\nDefault token extractor that warns about unimplemented flavors.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.compact_token_formatter","page":"Home","title":"StreamCallbacksExt.compact_token_formatter","text":"compact_token_formatter(tokens::TokenCounts, cost::Float64, elapsed::Union{Float64,Nothing}=nothing)\n\nFormat token statistics in a compact, emoji-decorated format: [üî§ in:X out:Y cache:(w:Z,r:W) üí∞$C ‚ö°Ô∏èTs]\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.calculate_cost","page":"Home","title":"StreamCallbacksExt.calculate_cost","text":"calculate_cost(cost_of_token_prompt, cost_of_token_generation, tokens::TokenCounts, cache_write_multiplier, cache_read_multiplier)\n\nCalculate the total cost for token usage based on the provided rates and multipliers.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacks.callback-Tuple{StreamCallbackWithTokencounts, StreamChunk}","page":"Home","title":"StreamCallbacks.callback","text":"callback(cb::StreamCallbackWithTokencounts, chunk::StreamChunk; kwargs...)\n\nProcess a stream chunk through the token-counting callback. This implementation:\n\nTracks timing information for inference\nExtracts and accumulates token counts\nCalculates costs based on model and token usage\nFormats and prints token statistics and content\n\nReturns a Dict with token counts if token information is available in the chunk.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.default_token_formatter","page":"Home","title":"StreamCallbacksExt.default_token_formatter","text":"default_token_formatter(tokens::TokenCounts, cost::Float64, elapsed::Union{Float64,Nothing}=nothing)\n\nFormat token statistics in a default text format: [in=X, out=Y, cache_in=Z, cache_read=W, $C, Ts]\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.StreamCallbackWithTokencounts","page":"Home","title":"StreamCallbacksExt.StreamCallbackWithTokencounts","text":"StreamCallbackWithTokencounts(; \n    out=stdout, \n    flavor=nothing, \n    chunks=StreamChunk[], \n    verbose=false,\n    throw_on_error=false,\n    kwargs=NamedTuple(),\n    total_tokens=TokenCounts(),\n    model=nothing,\n    token_formatter=default_token_formatter,\n    content_formatter=default_content_formatter,\n    timing=TimingInfo()\n)\n\nA stream callback that tracks token usage, costs, and timing information.\n\nArguments\n\nout: Output IO stream (default: stdout)\nflavor: Stream format handler (OpenAI/Anthropic)\nchunks: Vector to store stream chunks\nverbose: Enable verbose logging\nthrow_on_error: Whether to throw errors\nkwargs: Additional keyword arguments\ntotal_tokens: Accumulated token counts\nmodel: Model identifier\ntoken_formatter: Function to format token statistics\ncontent_formatter: Function to format streamed content\ntiming: Timing information\n\nExample\n\ncb = StreamCallbackWithTokencounts(     out = stdout,     flavor = StreamCallbacks.OpenAIStream() )\n\n\n\n\n\n","category":"type"}]
}
