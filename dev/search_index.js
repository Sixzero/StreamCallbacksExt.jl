var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Still-empty...-TODO.","page":"API Reference","title":"Still empty... TODO.","text":"","category":"section"},{"location":"#StreamCallbacksExt.jl","page":"Home","title":"StreamCallbacksExt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for StreamCallbacksExt.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Extension package for StreamCallbacks.jl with 3 things in mind:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Instant feedback as soon as usage package comes in.\nCached token informations\nCached token costs","category":"page"},{"location":"","page":"Home","title":"Home","text":"also adding timing for prompt processing time and inference time. Primarily designed to extend streaming capabilities in PromptingTools.jl.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Token counting for input, output, and cache operations\nCost calculation for different LLM providers (OpenAI, Anthropic)\nTiming information for inference and message processing\nCustomizable token and content formatters\nSupport for different stream flavors (OpenAI, Anthropic)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg Pkg.add(\"git@github.com:Sixzero/StreamCallbacksExt.jl.git\")","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using PromptingTools, StreamCallbacks, StreamCallbacksExt const PT = PromptingTools","category":"page"},{"location":"#Basic-streaming-with-token-counting","page":"Home","title":"Basic streaming with token counting","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     out = stdout,  # or any IO     flavor = StreamCallbacks.OpenAIStream(), ) msg = aigenerate(\"Write a story about a space cat\";     streamcallback=cb,     model=\"gpt4om\",     api_kwargs=(stream=true,) )","category":"page"},{"location":"#Token-Formatters","page":"Home","title":"Token Formatters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Two built-in formatters are provided:","category":"page"},{"location":"#Default-Text-Format","page":"Home","title":"Default Text Format","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream(),     tokenformatter = defaulttokenformatter ) [in=10, out=20, cachein=2, cache_read=5, $0.0015, 1.234s]","category":"page"},{"location":"#Compact-Emoji-Format","page":"Home","title":"Compact Emoji Format","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream(),     tokenformatter = compacttoken_formatter ) [ðŸ”¤ in:10 out:20 cache:(w:2,r:5) ðŸ’°$0.0015 âš¡ï¸1.234s]","category":"page"},{"location":"#Custom-Formatter","page":"Home","title":"Custom Formatter","text":"","category":"section"},{"location":"#Create-your-own-formatter","page":"Home","title":"Create your own formatter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"myformatter(tokens, cost, elapsed) = \"Words: $(tokens.output/1.3) Cost: \\$$(round(cost; digits=4))\" cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream(),     tokenformatter = my_formatter )","category":"page"},{"location":"#Types","page":"Home","title":"Types","text":"","category":"section"},{"location":"#TokenCounts","page":"Home","title":"TokenCounts","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tracks token usage across different categories:","category":"page"},{"location":"","page":"Home","title":"Home","text":"input: Number of new tokens in the prompt (excluding cached tokens)\noutput: Number of generated tokens in the response\ncache_write: Number of tokens written to cache\ncache_read: Number of tokens read from cache","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: Total input tokens = input + cachewrite + cacheread","category":"page"},{"location":"#RunInfo","page":"Home","title":"RunInfo","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tracks timing-related information during the streaming process:","category":"page"},{"location":"","page":"Home","title":"Home","text":"creation_time: When the callback was created\ninference_start: When the model started processing\nlast_message_time: Timestamp of the last received message\nstop_sequence: The sequence that caused the generation to stop (if any)","category":"page"},{"location":"#StreamCallbackWithTokencounts","page":"Home","title":"StreamCallbackWithTokencounts","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Main callback type that implements token counting, cost tracking, and timing: StreamCallbackWithTokencounts(;     out = stdout,              # Output IO stream     flavor = nothing,          # Stream format handler (OpenAI/Anthropic)     totaltokens = TokenCounts(), # Accumulated token counts     model = nothing,           # Model identifier     tokenformatter = defaulttokenformatter,     contentformatter = defaultcontent_formatter,     timing = RunInfo() )","category":"page"},{"location":"#Cost-Calculation","page":"Home","title":"Cost Calculation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package automatically calculates costs based on the model and provider:","category":"page"},{"location":"#OpenAI-models","page":"Home","title":"OpenAI models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.OpenAIStream() )","category":"page"},{"location":"#Anthropic-models","page":"Home","title":"Anthropic models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"cb = StreamCallbackWithTokencounts(     flavor = StreamCallbacks.AnthropicStream() )","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each provider has specific cache multipliers for cost calculation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"OpenAI: cachewrite: 1.0x, cacheread: 0.5x\nAnthropic: cachewrite: 1.25x, cacheread: 0.1x","category":"page"},{"location":"#Dependencies","page":"Home","title":"Dependencies","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StreamCallbacks.jl\nPromptingTools.jl","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TokenCounts\nStreamCallbacksExt.extract_model(::StreamCallbacks.AnthropicStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_model(::StreamCallbacks.OpenAIStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_model(::StreamCallbacks.AbstractStreamFlavor, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.get_cost(::StreamCallbacks.OpenAIStream, ::String, ::Any)\nStreamCallbacksExt.get_cost(::StreamCallbacks.AnthropicStream, ::String, ::Any)\nStreamCallbacksExt.RunInfo\nStreamCallbacksExt.default_content_formatter\nStreamCallbacksExt.extract_tokens(::StreamCallbacks.AnthropicStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_tokens(::StreamCallbacks.OpenAIStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_tokens(::StreamCallbacks.AbstractStreamFlavor, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.compact_token_formatter\nStreamCallbacksExt.calculate_cost\nStreamCallbacks.callback(::StreamCallbackWithTokencounts, ::StreamCallbacks.StreamChunk)\nStreamCallbacksExt.default_token_formatter\nStreamCallbacksExt.StreamCallbackWithTokencounts\nStreamCallbacksExt.StreamCallbackWithHooks\nStreamCallbacksExt.format_user_message\nStreamCallbacksExt.format_error_message\nStreamCallbacksExt.format_info_message\nStreamCallbacksExt.format_ai_message\nStreamCallbacksExt.extract_stop_sequence(::StreamCallbacks.OpenAIStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_stop_sequence(::StreamCallbacks.AnthropicStream, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.extract_stop_sequence(::StreamCallbacks.AbstractStreamFlavor, ::StreamCallbacks.AbstractStreamChunk)\nStreamCallbacksExt.handle_token_metadata(::StreamCallbacks.OpenAIStream, ::Any, ::Any, ::Any, ::Any)\nStreamCallbacksExt.handle_token_metadata(::StreamCallbacks.AnthropicStream, ::Any, ::Any, ::Any, ::Any)\nStreamCallbacksExt.handle_token_metadata(::StreamCallbacks.AbstractStreamFlavor, ::Any, ::Any, ::Any, ::Any)","category":"page"},{"location":"#StreamCallbacksExt.TokenCounts","page":"Home","title":"StreamCallbacksExt.TokenCounts","text":"TokenCounts(; input=0, output=0, cache_write=0, cache_read=0)\n\nTracks token usage across different categories:\n\ninput: Number of new tokens in prompt (excluding cached tokens)\noutput: Number of generated tokens in response\ncache_write: Number of tokens written to cache\ncache_read: Number of tokens read from cache\n\nNote: Total input tokens = input + cachewrite + cacheread\n\n\n\n\n\n","category":"type"},{"location":"#StreamCallbacksExt.extract_model-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_model","text":"extract_model(::StreamCallbacks.AnthropicStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract model identifier from Anthropic stream chunks, specifically from message_start events.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_model-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_model","text":"extract_model(::StreamCallbacks.OpenAIStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract model identifier from OpenAI stream chunks.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_model-Tuple{StreamCallbacks.AbstractStreamFlavor, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_model","text":"extract_model(::StreamCallbacks.AbstractStreamFlavor, chunk::StreamCallbacks.AbstractStreamChunk)\n\nDefault model extractor that warns about unimplemented flavors.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.get_cost-Tuple{OpenAIStream, String, Any}","page":"Home","title":"StreamCallbacksExt.get_cost","text":"get_cost(::StreamCallbacks.OpenAIStream, model::String, tokens)\n\nCalculate costs for OpenAI models with their specific cache multipliers.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.get_cost-Tuple{AnthropicStream, String, Any}","page":"Home","title":"StreamCallbacksExt.get_cost","text":"get_cost(::StreamCallbacks.AnthropicStream, model::String, tokens)\n\nCalculate costs for Anthropic models with their specific cache multipliers.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.RunInfo","page":"Home","title":"StreamCallbacksExt.RunInfo","text":"RunInfo(; creation_time=time(), inference_start=nothing, last_message_time=nothing, stop_sequence=nothing)\n\nTracks run statistics and metadata during the streaming process.\n\nFields\n\ncreation_time: When the callback was created\ninference_start: When the model started processing\nlast_message_time: Timestamp of the last received message\nstop_sequence: The sequence that caused the generation to stop (if any)\n\n\n\n\n\n","category":"type"},{"location":"#StreamCallbacksExt.default_content_formatter","page":"Home","title":"StreamCallbacksExt.default_content_formatter","text":"default_content_formatter(text::AbstractString)\n\nDefault content formatter that returns the text unchanged.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.extract_tokens-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_tokens","text":"extract_tokens(::StreamCallbacks.AnthropicStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract token counts from Anthropic stream chunks. Handles both message_start events with usage information and completion events with output tokens.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_tokens-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_tokens","text":"extract_tokens(::StreamCallbacks.OpenAIStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract token counts from OpenAI stream chunks. Handles:\n\nLegacy format with prompttokens and completiontokens\nCache hit/miss statistics\nDetailed token breakdowns (cachedtokens, audiotokens)\nEnd-of-stream combined usage statistics\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_tokens-Tuple{StreamCallbacks.AbstractStreamFlavor, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_tokens","text":"extract_tokens(::StreamCallbacks.AbstractStreamFlavor, chunk::StreamCallbacks.AbstractStreamChunk)\n\nDefault token extractor that warns about unimplemented flavors.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.compact_token_formatter","page":"Home","title":"StreamCallbacksExt.compact_token_formatter","text":"compact_token_formatter(tokens::TokenCounts, cost::Float64, elapsed::Union{Float64,Nothing}=nothing)\n\nFormat token statistics in a compact, emoji-decorated format: [ðŸ”¤ in:X out:Y cache:(w:Z,r:W) ðŸ’°$C âš¡ï¸Ts]\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.calculate_cost","page":"Home","title":"StreamCallbacksExt.calculate_cost","text":"calculate_cost(cost_of_token_prompt, cost_of_token_generation, tokens::TokenCounts, cache_write_multiplier, cache_read_multiplier)\n\nCalculate the total cost for token usage based on the provided rates and multipliers.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacks.callback-Tuple{StreamCallbackWithTokencounts, StreamChunk}","page":"Home","title":"StreamCallbacks.callback","text":"callback(cb::StreamCallbackWithTokencounts, chunk::StreamChunk; kwargs...)\n\nProcess a stream chunk through the token-counting callback. This implementation:\n\nTracks timing information for inference\nExtracts and accumulates token counts\nCalculates costs based on model and token usage\nFormats and prints token statistics and content\n\nReturns a Dict with token counts if token information is available in the chunk.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.default_token_formatter","page":"Home","title":"StreamCallbacksExt.default_token_formatter","text":"default_token_formatter(tokens::TokenCounts, cost::Float64, elapsed::Union{Float64,Nothing}=nothing)\n\nFormat token statistics in a default text format: [in=X, out=Y, cache_in=Z, cache_read=W, $C, Ts]\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.StreamCallbackWithTokencounts","page":"Home","title":"StreamCallbacksExt.StreamCallbackWithTokencounts","text":"StreamCallbackWithTokencounts(; \n    out=stdout, \n    flavor=nothing, \n    chunks=StreamChunk[], \n    verbose=false,\n    throw_on_error=false,\n    kwargs=NamedTuple(),\n    total_tokens=TokenCounts(),\n    model=nothing,\n    token_formatter=default_token_formatter,\n    content_formatter=default_content_formatter,\n    timing=RunInfo()\n)\n\nA stream callback that tracks token usage, costs, and timing information.\n\nArguments\n\nout: Output IO stream (default: stdout)\nflavor: Stream format handler (OpenAI/Anthropic)\nchunks: Vector to store stream chunks\nverbose: Enable verbose logging\nthrow_on_error: Whether to throw errors\nkwargs: Additional keyword arguments\ntotal_tokens: Accumulated token counts\nmodel: Model identifier\ntoken_formatter: Function to format token statistics\ncontent_formatter: Function to format streamed content\ntiming: Timing information\n\nExample\n\ncb = StreamCallbackWithTokencounts(     out = stdout,     flavor = StreamCallbacks.OpenAIStream() )\n\n\n\n\n\n","category":"type"},{"location":"#StreamCallbacksExt.StreamCallbackWithHooks","page":"Home","title":"StreamCallbacksExt.StreamCallbackWithHooks","text":"StreamCallbackWithHooks(; kwargs...)\n\nA stream callback that combines token counting with customizable hooks for various events.\n\nFields\n\nout: Output IO stream (default: stdout)\nflavor: Stream format handler (OpenAI/Anthropic)\nchunks: Vector to store stream chunks\nverbose: Enable verbose logging\nthrow_on_error: Whether to throw errors\nkwargs: Additional keyword arguments\ntotal_tokens: Accumulated token counts\nmodel: Model identifier\ntoken_formatter: Function to format token statistics\ntiming: Timing information\n\nHooks\n\ncontent_formatter: Function to process and format content text\non_meta_usr: Handler for user token counts/metadata\non_meta_ai: Handler for AI token counts/metadata\non_error: Error handler\non_done: Completion handler\non_start: Start handler\n\nExample\n\ncb = StreamCallbackWithHooks(\n    on_meta_ai = (tokens, cost, elapsed) -> println(\"AI: $(tokens.output) tokens\")\n)\n\n\n\n\n\n","category":"type"},{"location":"#StreamCallbacksExt.format_user_message","page":"Home","title":"StreamCallbacksExt.format_user_message","text":"format_user_message(tokens::TokenCounts, cost::Float64, elapsed::Union{Float64,Nothing}=nothing) -> String\n\nFormat user message with token counts in yellow color.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.format_error_message","page":"Home","title":"StreamCallbacksExt.format_error_message","text":"format_error_message(e) -> String\n\nFormat error message in red color.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.format_info_message","page":"Home","title":"StreamCallbacksExt.format_info_message","text":"format_info_message(msg) -> String\n\nFormat info messages in blue color.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.format_ai_message","page":"Home","title":"StreamCallbacksExt.format_ai_message","text":"format_ai_message(tokens::TokenCounts, cost::Float64, elapsed::Union{Float64,Nothing}=nothing) -> String\n\nFormat AI message with token counts in green color.\n\n\n\n\n\n","category":"function"},{"location":"#StreamCallbacksExt.extract_stop_sequence-Tuple{OpenAIStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_stop_sequence","text":"extract_stop_sequence(::StreamCallbacks.OpenAIStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract stop sequence from OpenAI stream chunks. Handles both delta.stopsequence and finishreason=\"stop\".\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_stop_sequence-Tuple{AnthropicStream, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_stop_sequence","text":"extract_stop_sequence(::StreamCallbacks.AnthropicStream, chunk::StreamCallbacks.AbstractStreamChunk)\n\nExtract stop sequence from Anthropic stream chunks.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.extract_stop_sequence-Tuple{StreamCallbacks.AbstractStreamFlavor, StreamCallbacks.AbstractStreamChunk}","page":"Home","title":"StreamCallbacksExt.extract_stop_sequence","text":"extract_stop_sequence(::StreamCallbacks.AbstractStreamFlavor, chunk::StreamCallbacks.AbstractStreamChunk)\n\nDefault stop sequence extractor that returns nothing.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.handle_token_metadata-Tuple{OpenAIStream, Vararg{Any, 4}}","page":"Home","title":"StreamCallbacksExt.handle_token_metadata","text":"Handle token metadata for OpenAI streams by calling both user and AI handlers.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.handle_token_metadata-Tuple{AnthropicStream, Vararg{Any, 4}}","page":"Home","title":"StreamCallbacksExt.handle_token_metadata","text":"Handle token metadata for Anthropic streams by dispatching based on token type.\n\n\n\n\n\n","category":"method"},{"location":"#StreamCallbacksExt.handle_token_metadata-Tuple{StreamCallbacks.AbstractStreamFlavor, Vararg{Any, 4}}","page":"Home","title":"StreamCallbacksExt.handle_token_metadata","text":"Default token metadata handler for other stream flavors.\n\n\n\n\n\n","category":"method"}]
}
